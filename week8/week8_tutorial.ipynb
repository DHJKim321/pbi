{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c203ff",
   "metadata": {},
   "source": [
    "### <font color='darkblue'>Week 8 Computing Lab</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eee5df",
   "metadata": {},
   "source": [
    "#### Working with Onotlogies and Functional Enrichment Analysis\n",
    "\n",
    "##### (and a little bit of network analysis..)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d17ac3",
   "metadata": {},
   "source": [
    "##### <font color='darkblue'>Introduction</font>\n",
    "In this computing lab weâ€™re going to be putting together what we've learned about biological databases and ontologies to do some summary analysis of genes invovled in the \"Dopaminergnic Synapse\" pathway. These can all be done using the KEGG and String-DB websites directly but we will show here that there is much greater power and flexibility available when you start using programmatic methods to carefully control your analyses.\n",
    "\n",
    "##### <font color='darkblue'>Learning Outcomes</font>\n",
    "After this tutorial you should be comfortable with:\n",
    "- Retrieving pathway information from KEGG\n",
    "- converting between accession IDs of different databases\n",
    "- Retrieving protein-protein interaction and network data from String-DB\n",
    "- Automating these processes using APIs and the NetworkX, and GSEAPy python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513cee1",
   "metadata": {},
   "source": [
    "## Step 1 - Setting up the Environment & Retrieving Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f012cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up the Programming Environment\n",
    "# %pip install networkx\n",
    "# %pip install gseapy\n",
    "\n",
    "# import modules for use in the notebook\n",
    "\n",
    "# handling www based requests (like APIs)\n",
    "import urllib as ul\n",
    "\n",
    "# standard Python data handling modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# working with networks\n",
    "import networkx as nx\n",
    "\n",
    "# working with geen set enrichment analysis (GSEA)\n",
    "import gseapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4336f",
   "metadata": {},
   "source": [
    "For Step1 you can either use the fully automated approcah using Steps 1a, and 1b, or working from files you generate from the KEGG and String-DB websites as described in Step 1c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a96a2",
   "metadata": {},
   "source": [
    "### Step 1a - Automating Download of KEGG Pathway Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee180b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we fetch the list of human pathways available at KEGG.\n",
    "\n",
    "human_pathways = pd.read_csv(ul.request.urlopen('http://rest.kegg.jp/list/pathway/hsa'),sep='\\t',header=0,names=['kegg_id','pathway_name'])\n",
    "human_pathways.head()\n",
    "\n",
    "# We specifically want the pathway data for the \"Dopaminergic Synapse\" pathway.\n",
    "pathway_info = human_pathways[human_pathways['pathway_name'].str.match('Dopaminergic synapse')]['kegg_id']\n",
    "\n",
    "# extract the exact pathway accession\n",
    "pathway_id = pathway_info.values[0].split(':')[1]\n",
    "\n",
    "print(pathway_id)\n",
    "\n",
    "# pull the pathway directly from KEGG, note we are saving this to a file 'dop_synapse.txt' that we will use later\n",
    "ul.request.urlretrieve('http://rest.kegg.jp/get/'+pathway_id,'dop_synapse.txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e89606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use this file which contains the full pathway details including the gene names.\n",
    "\n",
    "# open the file\n",
    "dop_file = open('dop_synapse.txt','r')\n",
    "\n",
    "# I wanted to show you some basic python parsing and a simple for loop with a conditional in to demonstrate how you can quickly build simple parsers.\n",
    "# There are quicker ways to do this, but this is a good learning example.\n",
    "\n",
    "# create an empty dataframe\n",
    "dop_df = pd.DataFrame()\n",
    "\n",
    "# set a flag for our parser\n",
    "flag=0\n",
    "\n",
    "# work through the text file one line at a time\n",
    "for line in dop_file:\n",
    "    # find the start of the gene entries\n",
    "    if 'GENE' in line:\n",
    "        # add the first gene tp the dataframe\n",
    "        dop_df = dop_df.append(pd.Series(line.strip('GENE').strip().split('  ')),ignore_index=True)\n",
    "        # set the flag to 1, we are in the gene section of the file\n",
    "        flag = 1\n",
    "    # stop when we reach the end of the section and escape the file\n",
    "    elif 'COMPOUND' in line:\n",
    "        break\n",
    "    # continue adding the genes to the dataframe\n",
    "    elif flag == 1:\n",
    "        dop_df = dop_df.append(pd.Series(line.strip().split('  ',2)),ignore_index=True)\n",
    "\n",
    "# close the file\n",
    "dop_file.close()\n",
    "\n",
    "# name the columns\n",
    "dop_df.columns = ['gene_id','description']\n",
    "\n",
    "# view the file\n",
    "dop_df.head()\n",
    "\n",
    "# you now have the gene_ids (NCBI EntrezIDs for the genes in the pathway)\n",
    "print('The Dopaminergic Synapse pathway has '+str(dop_df.shape[0])+' genes in it.\\n')\n",
    "\n",
    "# show the gene_ids\n",
    "print(dop_df['gene_id'].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cede081",
   "metadata": {},
   "source": [
    "### Step 1b - Automating Retrieval of Protein-Protein Interactions from STRING\n",
    "\n",
    "The details of the String-DB API can be found here - [https://string-db.org/help/api/](https://string-db.org/help/api/)\n",
    "\n",
    "APIs have specific formats required for their query URLs and it getting these correct in your code can take a little time until you get used to them. In this case we need to concatenate (stitch together) our gene IDs using a '%0D' string. This is actually the encoding for a line-return which is in effect mimicking the one gene per line entry that you would paste into the web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e55045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a concatenated list of entrezIDs as strings\n",
    "# note we are taking integer gene_ids from the 'gene_id' column of the dataframe we generated above then using\n",
    "# the map function to convert each one into a string. The join function then concatenates them using the '%0D' string\n",
    "# to stitch them all together. This string will be used to help us build the API query URL.\n",
    "entrezIDs = '%0D'.join(map(str,dop_df['gene_id']))\n",
    "\n",
    "# pass the list of EntrezIDs to the String-DB API return the String-IDs\n",
    "# we first form the query url using the 'get_string_ids' API function which takes a list of identifiers and\n",
    "# converts them into the internal String-DB accession IDs. This massively speeds up the search and allows us to\n",
    "# search for more than 10 at once which is an API restriction for other API functions if String-DB internal accessions \n",
    "# aren't used.\n",
    "query_url = 'https://string-db.org/api/tsv-no-header/get_string_ids?identifiers='+entrezIDs+'&format=only-ids'\n",
    "\n",
    "# use the urllib library to retrieve the String-DB internal IDs\n",
    "result = ul.request.urlopen(query_url).read().decode('utf-8')\n",
    "\n",
    "# now we want to query String-DB to retrieve interactions from this list of String-DB IDs\n",
    "# we create a concatenated list of stringdbIDs in much the same way as above for the Entrez Gene IDs\n",
    "stringdbIDs = '%0D'.join(result.splitlines())\n",
    "\n",
    "# again we build the query for interactions using the String-DB IDs\n",
    "query_url = 'https://string-db.org/api/tsv/network?identifiers='+stringdbIDs+'&species=9606'\n",
    "\n",
    "# again using urllib to retrieve the interactions these are returned in a standard tab delimied text format\n",
    "interactions = ul.request.urlopen(query_url).read().decode('utf-8').splitlines()\n",
    "\n",
    "# we need to split the result by these 'tabs' (\\t - is used to identfy them)\n",
    "int_test = [interaction.split('\\t') for interaction in interactions]\n",
    "\n",
    "# we extract the field names from the first row\n",
    "column_names = int_test[:1][0]\n",
    "\n",
    "# create a Pandas dataframe of the interaction data we have just retrieved from String-DB\n",
    "interactions_df = pd.DataFrame(int_test,columns=column_names)\n",
    "\n",
    "# delete the first row that held the fieldnames but we no longer need\n",
    "interactions_df = interactions_df.drop(labels=0,axis=0)\n",
    "\n",
    "# remove any duplicate rows\n",
    "final_interactions = interactions_df.drop_duplicates()\n",
    "\n",
    "# show the top of the protein-protein interaction table\n",
    "final_interactions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a57c3d",
   "metadata": {},
   "source": [
    "### Step 1c - Download Network from [STRING](https://string-db.org/cgi/input?sessionId=bIvRxxC0rWvS&input_page_show_search=on)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a50d2",
   "metadata": {},
   "source": [
    "In string upload dop_geneids.txt to string (make sure to click on multiple proteins)\n",
    "\n",
    "Select Homo Sapiens in Organism and click Search\n",
    "\n",
    "Quickly double check the mapping is correct before clicking continue\n",
    "\n",
    "You should now see your full protein-protein interaction network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534005f",
   "metadata": {},
   "source": [
    "What we would like to do is to identify clusters of enriched terms in this network\n",
    "\n",
    "First step is to cluster the network. This can be achieved in STRING by selecting Clusters -> MCL Clustering\n",
    "\n",
    "Change edge between clsuters to solid line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4a17f7",
   "metadata": {},
   "source": [
    "The next step is to download our clustered network. \n",
    "\n",
    "This is achieved by selceting Exports -> '... as tabular text output' \n",
    "\n",
    "This file is the full network with edges between every node. It is a '.tsv' format which can work with in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe8cf9",
   "metadata": {},
   "source": [
    "## Step 2 - Generating the Protein-Protein Interaction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e26d5",
   "metadata": {},
   "source": [
    "Next we are going to use the NetworkX Python library to create the protein-protein interaction network.\n",
    "\n",
    "NetworkX - Network Analysis in Python - [https://networkx.org/documentation/stable/index.html](https://networkx.org/documentation/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f006cb",
   "metadata": {},
   "source": [
    "### Step 2a - Working with the PPI dataframe created in Step 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac0457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column names of the dataframe\n",
    "print(final_interactions.columns)\n",
    "\n",
    " #Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# add all nodes\n",
    "G.add_nodes_from(set(final_interactions['preferredName_A']) | set(final_interactions['preferredName_B'])) \n",
    "\n",
    "# add the edges (connections) to the network\n",
    "edges = []\n",
    "for edge1 , edge2  in zip(final_interactions['preferredName_A'] , final_interactions['preferredName_B']) : #add all edge to the network\n",
    "    edges.append((edge1 , edge2 ))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# draw the network\n",
    "nx.draw(G , with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59388ee",
   "metadata": {},
   "source": [
    "### Step 2b - Working from the PPI file downloaded from the StringDB website in Step 1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the protein-protein interaction data from the downloaded String-DB tab separated file\n",
    "df = pd.read_csv('string_interactions.tsv' , delimiter='\\t')\n",
    "\n",
    "# show the top of the table\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cf30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column names of the dataframe\n",
    "# print(df.columns)\n",
    "\n",
    " #Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# add all nodes. Note we need to cover the scenario where node2 is connected to node1 only\n",
    "G.add_nodes_from(set(df['#node1']) |  set(df['node2']))\n",
    "\n",
    "# add the edges (connections) to the network\n",
    "edges = []\n",
    "for edge1 , edge2  in zip(df['#node1'] , df['node2']) :\n",
    "    edges.append((edge1 , edge2 ))\n",
    "\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# draw the network\n",
    "nx.draw(G , with_labels = True)\n",
    "\n",
    "#note how we keep our gene ids in the node names\n",
    "print(G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e578b669",
   "metadata": {},
   "source": [
    "## Step 3 - Gene Set Enrichement Analysis for our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427fcf8",
   "metadata": {},
   "source": [
    "### [GSEApy](https://gseapy.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "[GSEApy](https://gseapy.readthedocs.io/en/latest/index.html) is a library to perform gene set enrichment analysis (GSEA) in python. There are two methods to perform enrichment analysis - over representation analysis and GSEA. The main difference between the two is that GSEA assumes your input list of genes is ordered by the most representative genes in that list. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b9b15",
   "metadata": {},
   "source": [
    "### Enrichment Analysis Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2c1e60",
   "metadata": {},
   "source": [
    "In order to perform GSEA we need to impose an ordering on the list of gene ids. We want to order genes such that the genes which are most important to the above network are listed highest. \n",
    "\n",
    "Important genes to the network are ones that are central to the network structural i.e. removing these nodes will split the network or cause it to lose its structure. This means they tend to have highest number of connections and located in the centres of clusters.\n",
    "\n",
    "You can browse the available gene sets to perform enrichment analysis against using the GSEAPy package at the [Enrichr website](https://maayanlab.cloud/Enrichr/#libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b628d2",
   "metadata": {},
   "source": [
    "### Step 3a - Node Degree\n",
    "\n",
    "Node degree is simply the number of connections a node has. The higher the number of connections, the more important the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the genes (node names) by degree\n",
    "sorted_list = sorted(G.degree(), key=lambda item: item[1] , reverse=True)\n",
    "\n",
    "#extract just the node names from the sorted list\n",
    "sorted_genes = []\n",
    "for item in sorted_list :\n",
    "    sorted_genes.append(item[0])\n",
    "    \n",
    "# perform enrichment analysis using gsea\n",
    "enr = gseapy.enrichr(gene_list=sorted_genes,\n",
    "                 gene_sets=['KEGG_2021_Human'],\n",
    "                 organism='human', # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                 outdir=None, # don't write to disk\n",
    "                )\n",
    "\n",
    "enr.results.head(10) #return the top 10 hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57304098",
   "metadata": {},
   "source": [
    "### Step 3b - Closeness Centrality \n",
    "This is a measure of how close a node is to the center of the network. The closer a node is to the center the shorter its path to all other nodes and hence its more likely to be representative of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(nx.closeness_centrality(G).items(), key=lambda item: item[1] , reverse=True) \n",
    "#sort the genes (node names) by proximity to center\n",
    "\n",
    "sorted_genes = []\n",
    "for item in sorted_list : #extract just the node names from the sorted list\n",
    "    sorted_genes.append(item[0])\n",
    "    \n",
    "enr = gseapy.enrichr(gene_list=sorted_genes, # perform enrichment analysis using gsea\n",
    "                 gene_sets=['KEGG_2021_Human'],\n",
    "                 organism='human', # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                 outdir=None, # don't write to disk\n",
    "                )\n",
    "\n",
    "enr.results.head(10) #return the top 10 hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfc741",
   "metadata": {},
   "source": [
    "### Step 3c - Clustering Coefficient\n",
    "The clustering coefficient is a measure which combines centrality and degree. It measures the number of triangles a node can form ('the friend of my friend is my friend'). If a node has more common friends with other nodes it more likely to representative of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(nx.clustering(G).items(), key=lambda item: item[1] , reverse=True)\n",
    "#sort the genes (node names) by clustering coefficient\n",
    "\n",
    "sorted_genes = []\n",
    "for item in sorted_list : #extract just the node names from the sorted list\n",
    "    sorted_genes.append(item[0])\n",
    "    \n",
    "enr = gseapy.enrichr(gene_list=sorted_genes, # perform enrichment analysis using gsea\n",
    "                 gene_sets=['KEGG_2021_Human'],\n",
    "                 organism='human', # don't forget to set organism to the one you desired! e.g. Yeast\n",
    "                 outdir=None, # don't write to disk\n",
    "                )\n",
    "\n",
    "enr.results.head(10) #return the top 10 hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab1296b",
   "metadata": {},
   "source": [
    "## Step 4 - Exploring Gene Ontology Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17199cfa",
   "metadata": {},
   "source": [
    "In this step we use biomart which is an excellent service run at EBI-Ensembl that allows you to query and retrieve linked data for genomic data.\n",
    "\n",
    "The help for Biomart can be found here - [https://www.ensembl.org/info/data/biomart/how_to_use_biomart.html](https://www.ensembl.org/info/data/biomart/how_to_use_biomart.html)\n",
    "\n",
    "Biomart API functionality is nicely delivered through GSEApy and its use is described here - [https://gseapy.readthedocs.io/en/latest/gseapy_example.html#1.-Biomart-API](https://gseapy.readthedocs.io/en/latest/gseapy_example.html#1.-Biomart-API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5757a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the Entrez Gene IDs for the Dopaminergic Synapse pathway\n",
    "# **you could pull the pathway details as in Step 1a above.\n",
    "gene_ids = pd.read_table('dop_geneids.txt' , header=None , names=['gene_ids'])\n",
    "\n",
    "gene_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c20df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the GSEApy package also contains functions that allow you to use the EBI-Ensembl Biomart service\n",
    "# you can use this to directly query linked data for the genes, including Gene Ontology (GO) annotation data.\n",
    "from gseapy import Biomart\n",
    "\n",
    "# initiate a biomart connection\n",
    "bm = Biomart()\n",
    "\n",
    "# form a query for biomart from the Entrez Gene IDs\n",
    "queries = {'entrezgene_id' :list(gene_ids.values.reshape(1,-1)[0])}\n",
    "\n",
    "# execute the biomart query\n",
    "# NB that the oddly named 'name_1006' attribute is in fact the 'GO Term name' attribute\n",
    "# NB a nice trick for finding the correct attribute names is to use the website to create the query and then\n",
    "# NB click on the XML link at the top of the page, this will show you the 'Attribute name'.\n",
    "results = bm.query(dataset='hsapiens_gene_ensembl',\n",
    "                   attributes=['ensembl_gene_id', 'external_gene_name', 'entrezgene_id', 'go_id','go_linkage_type','name_1006'],\n",
    "                   filters=queries)\n",
    "\n",
    "# change the name to something more useful\n",
    "results.rename({'name_1006' : 'go_name'},axis=1,inplace=True)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391a24f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the top 20 most common GO terms\n",
    "top_20_GO = results['go_name'].value_counts()[:20];\n",
    "\n",
    "top_20_GO.plot.bar(xlabel='GO Term',ylabel='GO Term Frequency',title='Top 20 Most Frequent GO Annotations in the Dopaminergic Synapse Pathway');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to take extra advantage of the data lets break down the above by evidence code type\n",
    "annotations_by_evidence = pd.DataFrame(results[['go_name','go_linkage_type']].dropna())\n",
    "\n",
    "# we use the Pandas pivot_table function to count and sort the pairs go_term & evidnce code\n",
    "annotations_by_evidence = annotations_by_evidence.pivot_table(index=['go_name','go_linkage_type'],aggfunc='size').reset_index(name='frequency').sort_values(by='frequency',ascending=False)\n",
    "\n",
    "# just take the top 30 rows\n",
    "top_annotations = annotations_by_evidence[:30]\n",
    "\n",
    "# pivot to create a nice matrix for a stacked plot\n",
    "top_annotations = top_annotations.pivot(index='go_name',columns='go_linkage_type',values='frequency').fillna(0)\n",
    "\n",
    "# create a stacked barchart of the results\n",
    "top_annotations.plot.bar(xlabel='GO Term',\n",
    "    ylabel='GO Term Frequency',\n",
    "    title='Top Frequent GO Annotations by Evidence Code in the Dopaminergic Synapse Pathway',\n",
    "    stacked=True).legend(loc='best');\n",
    "\n",
    "# the nicely reshaped data frame used for the stacked plot\n",
    "top_annotations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795c76f",
   "metadata": {},
   "source": [
    "Check the function of the most common GO terms [here](https://www.ebi.ac.uk/QuickGO/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39682a1e",
   "metadata": {},
   "source": [
    "## Step5 - [PantherDB](http://www.pantherdb.org/)\n",
    "\n",
    "[PantherDB](http://www.pantherdb.org/) is another online tool which can be used to perform an enrichment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b82f0b",
   "metadata": {},
   "source": [
    "### Step 5a - Using the PantherDB Website for Enrichment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9124911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a unique list of Dopaminergic Synapse genes from our earlier dataframe\n",
    "dop_genes = list(dop_df['gene_id'].dropna().drop_duplicates())\n",
    "\n",
    "# write them to a file to upload to the PantherDB website\n",
    "with open('entrezgene_id.txt', 'w') as f:\n",
    "    f.write('\\n'.join(str(gene) for gene in dop_genes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f1acbc",
   "metadata": {},
   "source": [
    "Upload this file on the [PantherDB](http://www.pantherdb.org/) website\n",
    "\n",
    "Select Functional classification viewed in graphic charts bar chart and be sure to select \"Homo sapiens\" as species\n",
    "\n",
    "Do the GO terms match up to that found in PantherDB, why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55680ba1",
   "metadata": {},
   "source": [
    "### Step 5b - Using the PantherDB API for Enrichment Analysis\n",
    "\n",
    "PantherDB API details - [http://pantherdb.org/services/details.jsp](http://pantherdb.org/services/details.jsp)\n",
    "\n",
    "Functionality and Parameter testing - [http://pantherdb.org/services/openAPISpec.jsp](http://pantherdb.org/services/openAPISpec.jsp)\n",
    "\n",
    "This is quite hard work so you might decide it's simplest (and quicker) at this stage to use the website functionality above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37750871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results are returned in JSON format so we need to load a Python module to handle this\n",
    "import json\n",
    "\n",
    "# the PantherDB API offers this function to find out what annotated resources it has available\n",
    "query_url = 'http://pantherdb.org/services/oai/pantherdb/supportedannotdatasets'\n",
    "\n",
    "# execute the query\n",
    "result = ul.request.urlopen(query_url)\n",
    "\n",
    "# load the results returning a Python dictionary\n",
    "annotationSets = json.load(result)\n",
    "\n",
    "annotations = annotationSets['search']['annotation_data_sets']['annotation_data_type']\n",
    "\n",
    "# we can just iterate through these to see the annotation sources available\n",
    "for i in annotations:\n",
    "    print('Annotation Set Label = '+i['label']+', annotDataSet string to use below = '+i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6961befa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the list of Entrez Gene IDs generated above (entrezgene_id) create a query string for the API\n",
    "# these need to be comma separated\n",
    "genes = ','.join(map(str,dop_genes))\n",
    "\n",
    "# use the PantherDB API - NB that GO:0008150 is the accession for the \"Biological Process\" clade of the Gene Ontology from above\n",
    "query_url = \"http://pantherdb.org/services/oai/pantherdb/enrich/overrep?&geneInputList=\"+genes+\"&organism=9606&annotDataSet=GO:0008150&enrichmentTestType=FISHER&correction=FDR\"\n",
    "\n",
    "# capture the results (NB this returns in JSON format)\n",
    "result = ul.request.urlopen(query_url)\n",
    "\n",
    "# load the results from JSON to Python dictionary\n",
    "enrichment_result = json.load(result)\n",
    "\n",
    "# view the raw results\n",
    "print(enrichment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568e80a",
   "metadata": {},
   "source": [
    "We're now going to format that into something human readable. There are many ways to do this, but this is a quick and (fairly) simple solution. Please do feel free to try your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84428568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the actual result component\n",
    "results = enrichment_result['results']['result']\n",
    "\n",
    "# how long is the background list (in this case it is the default, the whole genome)\n",
    "print(len(results), \"terms in reference list\")\n",
    "\n",
    "# print some headers\n",
    "display_headers = [\"GO Term\", \"Expected\", \"Fold enrichment\", \"raw P value\", \"FDR\", \"Term label\"]\n",
    "print(\"\\t\".join(display_headers))\n",
    "\n",
    "# Sort in order of false discovery rate i.e. multiple testing correction\n",
    "results.sort(key=lambda x: x['fdr'], reverse=False)\n",
    "\n",
    "# iterate through all of the results and print out the biots we're interested in\n",
    "for r in results:\n",
    "    fdr = r['fdr']\n",
    "    if fdr < 0.05:\n",
    "        # Print result line\n",
    "        term_id = r['term'].get(\"id\")\n",
    "        if term_id is None:\n",
    "            term_id = \"\"\n",
    "        else:\n",
    "            print(\"\\t\".join([\n",
    "                term_id,\n",
    "                str(r['expected']),  # Convert float to string for printing\n",
    "                str(r['fold_enrichment']),\n",
    "                str(r['pValue']),\n",
    "                str(r['fdr']),\n",
    "                r['term'][\"label\"]\n",
    "            ])\n",
    "            )\n",
    "\n",
    "# that was quite painful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c79478e135452d4f8dcea3898ce85a4457be8d06848dc07bbec8d2854f4ceed7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
